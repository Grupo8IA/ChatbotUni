{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modelo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq3ziQeUmC5K"
      },
      "source": [
        "# MODELO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8IibjRfnwN9"
      },
      "source": [
        "### PREPARACIÓN DE LA DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No68nVceltdz"
      },
      "source": [
        "El proceso de preparación de la data para el modelo de entrenamiento consiste en obtener listas de vectores, cada vector contiene la sentencia de pregunta y la sentencia de respuesta. En este proceso también hacemos un mapeo de cada palabra a índices y de índice palabras, esto se almacenará en diccionarios, además de llevar el conteo de cuántas veces una palabra se repite. Esto nos permitirá obtener el índice que le corresponde a cada palabra y viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMpxC_Gqr6Nx",
        "outputId": "75920fad-b5c2-4212-ff1e-ea70019ecd23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/iA"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/.shortcut-targets-by-id/1ccQ0NRVtxcnMDQHOrldbGFQDfXPWk9Jj/iA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yAFs-CzWAp3",
        "outputId": "825a839f-f3db-4fc3-9592-4add62b12d3f"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20.txt\n",
            "BackwardForward.ipynb\n",
            "checkpoint-10ML-30ep-META-512bz-loss29-valoss41.pt\n",
            "checkpoint-10ML-30ep-META-512bz-loss30-valoss41-con-tildes.pt\n",
            "checkpoint-10ML-60ep-META-512bz-loss26-valoss46-con-tildes2.pt\n",
            "checkpoint-15ML-20EP.pt\n",
            "checkpoint-15ML-30ep-META-512bz-loss40-valoss54-con-tildes.pt\n",
            "checkpoint-20ML-15ep-data-METALW-512bz-con-tildes-300em-300h.pt\n",
            "checkpoint-20ML-25ep-data-METALW-512bz-con-tildes-300em-300h.pt\n",
            "checkpoint-25ML-13EP-300BS.pt\n",
            "checkpoint-25ML-15ep-data-METALW-512bz-con-tildes-200em-200h.pt\n",
            "checkpoint-25ML-25ep-data-METALW-512bz-con-tildes-200em-200h.pt\n",
            "checkpoint2.pt\n",
            "checkpoint-50ML-20EP.pt\n",
            "checkpoint.pt\n",
            "\u001b[0m\u001b[01;34mcontinnuacion\u001b[0m/\n",
            "cultura_general_1\n",
            "data_freider.txt\n",
            "\u001b[01;34mData_PC2\u001b[0m/\n",
            "final.txt\n",
            "GRL_Book.pdf\n",
            "METALW.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoFvqUsfkFGm"
      },
      "source": [
        "import torch\n",
        "from torch.jit import script\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "from io import open\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "MAX_LENGTH = 10 # Cantidad de palabras máxima por cada sentencia"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COocxGdvfALW"
      },
      "source": [
        "# Definiciones:\n",
        "#   Linea: Pares de sentencias separadas por un padding \\t\n",
        "#   Sentencia: Texto de la pregunta o la respuesta.\n",
        "#   Par: Un vector, cada vector contiene dos senticias: pregunta y la respuesta\n",
        "\n",
        "PAD_token = 0  # Token para rellenar las sentencias con una cantidad menor a MAX_LENGTH\n",
        "SOS_token = 1  # Token que indica el inicio de la sentencia\n",
        "EOS_token = 2  # Token que indica el final de la sentencia\n",
        "\n",
        "# Objeto Voc: \n",
        "#   Procesará cada sentencia de cada línea. \n",
        "#   Nos ayudará a generar una mapeo de cada palabra a indices (números)\n",
        "#   lo que permitirá obtener el índice que corresponde a cada palabra, la palabra que le\n",
        "#   corresponde a cada índice y la cantidad de veces que una palabra se repíte.\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {\"PAD\":PAD_token , \"SOS\":SOS_token , \"EOS\":EOS_token }\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Los 3 tokens inicializados SOS, EOS, PAD\n",
        "\n",
        "    def agregarSentencia(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.agregarPalabra(word)\n",
        "\n",
        "    def agregarPalabra(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def indiceDeSentencia(self, sentencia):\n",
        "        return [self.word2index[word] for word in sentencia.split(' ')] + [EOS_token]\n",
        "\n",
        "    def sentenciaDeIndice(self, indice):\n",
        "        return [self.index2word[idx] for idx in indice]\n",
        "\n",
        "    # Remueve las palabras que se repiten menos de una cierta cantidad de veces\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinicializamos los diccionarios\n",
        "        self.word2index = {\"PAD\":PAD_token , \"SOS\":SOS_token , \"EOS\":EOS_token }\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Los 3 tokens inicializados SOS, EOS, PAD\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.agregarPalabra(word)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltLAHNyz9K3F",
        "outputId": "a3dee3f6-86cf-4607-cb5a-c357af2502fa"
      },
      "source": [
        "# Función que normalizará cada sentencia\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        # c for c in unicodedata.normalize('NFD', s) # Normalizará y eliminará las tildes y la ñ\n",
        "        c for c in unicodedata.normalize('NFC', s) # Normalizará y mantiene las tildes y la ñ\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Normalizamos cada sentencia\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip()) # Normalizamos y pasamos todas las letras a minúsculas\n",
        "    # Pasamos cada sentencia a minúsculas, removemos los espacios y carácteres que no son letras excluyendo los números\n",
        "    s = re.sub(r\"([.¡!¿?])\", r\" \\1\", s)  # Mantenemos los signos interrogación y exclamación de apertura y cierre\n",
        "    s = re.sub(r\"[^A-zÁ-ú.¡!¿?0-9]+\", r\" \", s) # Mantenemos las tildes y números\n",
        "    # s = re.sub(r\"\\¿\", r\"¿ \", s) # Agregamos un espacio a los signos de interrogación para que se cuente como una palabra\n",
        "    # s = re.sub(r\"\\?\", r\" ?\", s) # Agregamos un espacio a los signos de interrogación para que se cuente como una palabra\n",
        "    # s = re.sub(r\"\\¡\", r\"¡ \", s) # Agregamos un espacio a los signos de exclamación para que se cuente como una palabra\n",
        "    # s = re.sub(r\"\\!\", r\" !\", s) # Agregamos un espacio a los signos de exclamación para que se cuente como una palabra\n",
        "    # s = re.sub(r\"\\s+\", r\" \", s).strip() # Eliminamos los espacios demás\n",
        "    # s = re.sub(r\"\\.\", r\"\", s).strip() # Eliminamos los puntos\n",
        "\n",
        "    # s = re.sub(r\"[^A-z.¡!¿?0-9]+\", r\" \", s) # Elimina tildes\n",
        "    s = re.sub(r\"\\.\", r\"\", s)\n",
        "    s = re.sub(r\"\\¿\\s+\", r\"¿\", s) # Elimina espacios alrededor del signo de interrogación\n",
        "    s = re.sub(r\"\\s+\\?\", r\"?\", s) # Elimina espacios alrededor del signo de interrogación\n",
        "    s = re.sub(r\"\\¡\\s\", r\"¡\", s) # Elimina espacios alrededor del signo de exclamación\n",
        "    s = re.sub(r\"\\!\\s\", r\"!\", s) # Elimina espacios alrededor del signo de exclamación\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip() # Elimina los espacios demás\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "# Leemos las lineas del archivo y devolvemos los pares y un objeto Voc\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Leyendo líneas...\")\n",
        "    # Leemos el archivo y devuelve una lista de líneas\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Dividimos cada linea en pares, normaliza normalizando cada sentencia\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    # Devuelve el objeto vocabulario y los pares\n",
        "    return voc, pairs\n",
        "\n",
        "# Retorna True si ambas sentencias en el par tienen una cantidad de palabras menores que MAX_LENGTH\n",
        "def filtrarPar(p):\n",
        "    # Las sentencias de entrada, necesitamos un espacio para el token SOS\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Filtra los pares usando la función filtrarPar\n",
        "def filtrarPares(pairs):\n",
        "    return [pair for pair in pairs if filtrarPar(pair)]\n",
        "\n",
        "# Usando las funciones definidas arriba generamos el diccionario que mapea de palabras a índices\n",
        "# devolverá el objeto voc y la lista de pares\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Empieza la preparación de la data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Se leyó {!s} pares de sentencias\".format(len(pairs)))\n",
        "    pairs = filtrarPares(pairs)\n",
        "    print(\"Filtrado {!s} pares de sentencias\".format(len(pairs)))\n",
        "    print(\"Contando las palabras...\")\n",
        "\n",
        "    for pair in pairs:\n",
        "        # Agregamos cada sentencia al objeto Voc para hacer el mapeo\n",
        "        voc.agregarSentencia(pair[0])\n",
        "        voc.agregarSentencia(pair[1])\n",
        "        \n",
        "    print(\"Cantidad total de palabras:\", voc.num_words)\n",
        "    \n",
        "    \n",
        "    return voc, pairs\n",
        "\n",
        "\n",
        "\n",
        "save_dir = \"./\"\n",
        "datafile = \"METALW.txt\"\n",
        "corpus = \"./\"\n",
        "corpus_name = \"dataf_s2s\"\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "\n",
        "# Imprimimos una muestra de la data para verificar su estructura\n",
        "print(\"\\npares:\")\n",
        "\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)\n",
        "\n",
        "# Fuente: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chatbot%20tutorial"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empieza la preparación de la data ...\n",
            "Leyendo líneas...\n",
            "Se leyó 319593 pares de sentencias\n",
            "Filtrado 175166 pares de sentencias\n",
            "Contando las palabras...\n",
            "Cantidad total de palabras: 40305\n",
            "\n",
            "pares:\n",
            "['hola ¿en qué puedo ayudarle?', 'sí']\n",
            "['sí', '¿cómo puedo ser de ayuda?']\n",
            "['¿cómo puedo ser de ayuda?', 'quiero saber sobre la política que tengo']\n",
            "['quiero saber sobre la política que tengo', 'bien ¿puedo conseguir tu nombre por favor?']\n",
            "['bien ¿puedo conseguir tu nombre por favor?', '¿cubre los daños causados por el agua?']\n",
            "['hola ¿en qué puedo ayudarle?', 'tengo una pregunta sobre mi política']\n",
            "['tengo una pregunta sobre mi política', 'seguro ¿puedes decirme tu número de póliza?']\n",
            "['seguro ¿puedes decirme tu número de póliza?', '3425512']\n",
            "['3425512', 'bien tengo tu póliza aquí ¿cuál es tu pregunta?']\n",
            "['bien tengo tu póliza aquí ¿cuál es tu pregunta?', 'cubre los daños causados por el agua?']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}